{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import html\n",
    "import base64\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam  \n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    get_scheduler\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Generator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:26:13.764810Z",
     "iopub.status.busy": "2025-06-12T18:26:13.764270Z",
     "iopub.status.idle": "2025-06-12T18:26:13.769762Z",
     "shell.execute_reply": "2025-06-12T18:26:13.769054Z",
     "shell.execute_reply.started": "2025-06-12T18:26:13.764775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'auto-annotation/Dataset' \n",
    "DATASET_ROOT = os.path.join('/kaggle/input', DATASET_NAME)\n",
    "IMAGE_DIR = os.path.join(DATASET_ROOT, 'Images')\n",
    "\n",
    "# ADD IF LABELS ARE AVAILABLE ELSE NONE\n",
    "LABEL_DIR = os.path.join(DATASET_ROOT, 'Labels')\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "SAVING_LABELLED_ANNOTATION = \"/kaggle/working/annotated_images\"\n",
    "\n",
    "CLASS_MAP = {\"car\": 0, \"bottle\": 1, \"dog\": 2} #adjust as needed\n",
    "CLASS_MAP_LABEL= {\n",
    "    0: \"Car\",\n",
    "    1: \"Bottle\",\n",
    "    2: \"Dog\"\n",
    "}\n",
    "\n",
    "TEXT_INPUT = \"car, dog, cat\" # add as per needed the prompt/classes to label\n",
    "\n",
    "LOG_FILE_PATH = \"/kaggle/working/florence_output_log.txt\" # to save the logs of prediction\n",
    "SAVE_LABEL_PATH = \"/kaggle/working/generated_labels\" #Directory path to save the predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:05:45.497485Z",
     "iopub.status.busy": "2025-06-12T18:05:45.496566Z",
     "iopub.status.idle": "2025-06-12T18:05:45.516941Z",
     "shell.execute_reply": "2025-06-12T18:05:45.516215Z",
     "shell.execute_reply.started": "2025-06-12T18:05:45.497451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"--- Checking for CUDA availability ---\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "print(\"------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- 1. Define the transform pipeline ---\n",
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.Resize((250, 250)),\n",
    "    transforms.ToTensor(),         \n",
    "    transforms.Normalize([0.565, 0.566, 0.529], \n",
    "                         [0.276, 0.276, 0.298])\n",
    "])\n",
    "\n",
    "# --- 2. Define the Custom Dataset Class ---\n",
    "class ImageAnnotationDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir=None, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the custom dataset.\n",
    "\n",
    "        Args:\n",
    "            image_dir (string): Path to the directory containing image files.\n",
    "            label_dir (string, optional): Path to the directory containing YOLO format label files (.txt).\n",
    "                                          If None, no labels will be loaded for evaluation.\n",
    "            transform (callable, optional): Optional transform to be applied to the images.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) \n",
    "                                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])\n",
    "        \n",
    "        if not self.image_files:\n",
    "            print(f\"Warning: No image files found in {image_dir}. Please check your path and file extensions.\")\n",
    "            \n",
    "        print(f\"Dataset initialized with {len(self.image_files)} images.\")\n",
    "        if self.label_dir:\n",
    "            print(f\"Label directory provided: {self.label_dir}\")\n",
    "        else:\n",
    "            print(\"No label directory provided. Annotations will not be loaded from files.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item (image, annotations, original size, img_name) from the dataset at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - transformed_image (torch.Tensor): The image after applying transformations.\n",
    "                - annotations (list of dicts): A list of dictionaries, where each dictionary\n",
    "                                              represents an object and contains its 'class_id'\n",
    "                                              and 'bbox_normalized_xyxy' (xmin, ymin, xmax, ymax).\n",
    "                                              This will be an empty list if labels are not provided/found.\n",
    "                - original_size (tuple): The original (width, height) of the image before transforms.\n",
    "        \"\"\"\n",
    "        # Construct the full path to the image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load the image using PIL and convert to RGB (important for consistent channel order)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_width, original_height = image.size # Store original dimensions\n",
    "\n",
    "        # Initialize annotations as an empty list; populate if labels are available\n",
    "        annotations = []\n",
    "        if self.label_dir: # Check if a label directory was provided\n",
    "            # Construct the full path to the corresponding YOLO label file.\n",
    "            # Assumes label files have the same base name as images but with a .txt extension.\n",
    "            label_name = img_name.rsplit('.', 1)[0] + '.txt'\n",
    "            label_path = os.path.join(self.label_dir, label_name)\n",
    "\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = list(map(float, line.strip().split()))\n",
    "                        class_id = int(parts[0])\n",
    "                        # YOLO format: class_id x_center y_center width height (all normalized 0-1)\n",
    "                        cx, cy, w, h = parts[1:]\n",
    "\n",
    "                        # Convert normalized cx, cy, w, h to normalized xmin, ymin, xmax, ymax\n",
    "                        xmin = cx - (w / 2)\n",
    "                        ymin = cy - (h / 2)\n",
    "                        xmax = cx + (w / 2)\n",
    "                        ymax = cy + (h / 2)\n",
    "                        \n",
    "                        # Store the parsed annotation\n",
    "                        annotations.append({\n",
    "                            'class_id': class_id,\n",
    "                            'bbox_normalized_xyxy': [xmin, ymin, xmax, ymax]\n",
    "                        })\n",
    "\n",
    "\n",
    "        # Apply transformations to the image if a transform pipeline is provided\n",
    "        if self.transform:\n",
    "            transformed_image = self.transform(image)\n",
    "        else:\n",
    "            # If no transform is given, convert the PIL Image to a PyTorch Tensor\n",
    "            transformed_image = transforms.ToTensor()(image)\n",
    "\n",
    "        # Return the transformed image, the parsed annotations, and the original image size.\n",
    "        # The original image size is crucial for later converting normalized bounding boxes back to pixel coordinates.\n",
    "        return transformed_image, annotations, (original_width, original_height), img_name\n",
    "\n",
    "\n",
    "# --- Custom Collate Function for DataLoader ---\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to handle variable-length annotations.\n",
    "    It stacks images into a tensor and keeps annotations and original sizes\n",
    "    as lists of their respective items.\n",
    "    \"\"\"\n",
    "    # Each item in 'batch' is (transformed_image, annotations, original_size)\n",
    "    \n",
    "    # Stack the images into a single tensor\n",
    "    # torch.stack will combine (C, H, W) tensors into (B, C, H, W)\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    \n",
    "    # Collect annotations and original sizes as lists\n",
    "    # These are lists of lists/tuples, maintaining their individual structure per image\n",
    "    annotations = [item[1] for item in batch]\n",
    "    original_sizes = [item[2] for item in batch]\n",
    "    img_names = [item[3] for item in batch]\n",
    "    \n",
    "    return images, annotations, original_sizes, img_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:08:39.024385Z",
     "iopub.status.busy": "2025-06-12T18:08:39.024130Z",
     "iopub.status.idle": "2025-06-12T18:08:39.084000Z",
     "shell.execute_reply": "2025-06-12T18:08:39.083324Z",
     "shell.execute_reply.started": "2025-06-12T18:08:39.024367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Main Running Part ---\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Initializing and Preparing Dataset ---\")\n",
    "try:\n",
    "    # Initialize the dataset with your image and label directories\n",
    "    dataset_for_annotation = ImageAnnotationDataset(\n",
    "        image_dir=IMAGE_DIR, \n",
    "        label_dir=LABEL_DIR, \n",
    "        transform=transform_pipeline\n",
    "    )\n",
    "\n",
    "    if len(dataset_for_annotation) > 0:\n",
    "        batch_size = BATCH_SIZE\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset_for_annotation, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=0,\n",
    "            collate_fn=custom_collate_fn \n",
    "        )\n",
    "\n",
    "        print(f\"\\nDataLoader created with batch size: {batch_size}\")\n",
    "        print(f\"Total batches to process: {len(data_loader)}\")\n",
    "\n",
    "        # List to hold processed data from the DataLoader\n",
    "        all_processed_data = []\n",
    "\n",
    "        print(\"\\n--- Preprocessing images... ---\")\n",
    "        for i, (images, annotations, original_sizes, img_names) in enumerate(tqdm(data_loader, desc=\"Preprocessing Images\")):\n",
    "            \n",
    "            all_processed_data.append({\n",
    "                \"images\": images,\n",
    "                \"annotations\": annotations,\n",
    "                \"original_sizes\": original_sizes, \n",
    "                \"img_names\": img_names\n",
    "            })\n",
    "        \n",
    "        print(f\"\\n--- Preprocessing complete for {len(dataset_for_annotation)} images ---\")\n",
    "        print(f\"Number of batches collected: {len(all_processed_data)}\") # Will be 1 if batch_size=len(dataset)\n",
    "\n",
    "    else:\n",
    "        print(\"The dataset is empty. Please verify your image directory path and contents.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: A required directory or file was not found. Please check paths. {e}\")\n",
    "    print(f\"Expected image directory: {IMAGE_DIR}\")\n",
    "    if LABEL_DIR:\n",
    "        print(f\"Expected label directory: {LABEL_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing current images with labels if labels are avaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:12:38.030753Z",
     "iopub.status.busy": "2025-06-12T18:12:38.030215Z",
     "iopub.status.idle": "2025-06-12T18:12:38.601735Z",
     "shell.execute_reply": "2025-06-12T18:12:38.601001Z",
     "shell.execute_reply.started": "2025-06-12T18:12:38.030726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def visualize_annotations_from_dataloader_and_save(dataloader, class_map, save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize YOLO-style annotations from a DataLoader using Matplotlib.\n",
    "\n",
    "    Args:\n",
    "        dataloader: Yields (images, annotations_batch, original_sizes, filenames).\n",
    "        class_map: Dictionary mapping class ID to class name.\n",
    "        save_dir: If provided, saves the annotated image with original filename.\n",
    "    \"\"\"\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for images, annotations_batch, original_sizes, filenames in dataloader:\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            image_tensor = images[i].cpu()\n",
    "            annotation_list = annotations_batch[i]\n",
    "            orig_w, orig_h = original_sizes[i]\n",
    "            filename = filenames[i]\n",
    "\n",
    "            # Denormalize image\n",
    "            mean = torch.tensor([0.565, 0.566, 0.529]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.276, 0.276, 0.298]).view(3, 1, 1)\n",
    "            image_tensor = image_tensor * std + mean\n",
    "            image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "            image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "            image_np = cv2.resize(image_np, (orig_w, orig_h))\n",
    "\n",
    "            # Create plot\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            ax.imshow(image_np)\n",
    "\n",
    "            if not annotation_list:\n",
    "                print(f\"No annotations for {filename}\")\n",
    "\n",
    "            for ann in annotation_list:\n",
    "                cls_id = ann['class_id']\n",
    "                xmin, ymin, xmax, ymax = ann['bbox_normalized_xyxy']\n",
    "                x1 = int(xmin * orig_w)\n",
    "                y1 = int(ymin * orig_h)\n",
    "                x2 = int(xmax * orig_w)\n",
    "                y2 = int(ymax * orig_h)\n",
    "\n",
    "                label = class_map.get(cls_id, f\"Class {cls_id}\")\n",
    "                rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                         linewidth=2, edgecolor='lime', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, max(y1 - 5, 5), label, color='black', fontsize=9,\n",
    "                        bbox=dict(facecolor='yellow', alpha=0.7))\n",
    "\n",
    "            ax.set_title(filename)\n",
    "            ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if save_dir:\n",
    "                save_path = os.path.join(save_dir, filename)\n",
    "                plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)\n",
    "                plt.show()\n",
    "                # print(f\"Saved: {save_path}\")\n",
    "            else:\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:15:09.862005Z",
     "iopub.status.busy": "2025-06-12T18:15:09.861699Z",
     "iopub.status.idle": "2025-06-12T18:15:17.812034Z",
     "shell.execute_reply": "2025-06-12T18:15:17.811392Z",
     "shell.execute_reply.started": "2025-06-12T18:15:09.861953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_annotations_from_dataloader_and_save(data_loader, class_map = CLASS_MAP, save_dir=SAVING_LABELLED_ANNOTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:15:51.165897Z",
     "iopub.status.busy": "2025-06-12T18:15:51.165630Z",
     "iopub.status.idle": "2025-06-12T18:22:13.611528Z",
     "shell.execute_reply": "2025-06-12T18:22:13.610952Z",
     "shell.execute_reply.started": "2025-06-12T18:15:51.165878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "%matplotlib inline  \n",
    "\n",
    "model_id = 'microsoft/Florence-2-large'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:22:13.613482Z",
     "iopub.status.busy": "2025-06-12T18:22:13.612768Z",
     "iopub.status.idle": "2025-06-12T18:22:13.627714Z",
     "shell.execute_reply": "2025-06-12T18:22:13.627026Z",
     "shell.execute_reply.started": "2025-06-12T18:22:13.613455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define this somewhere globally in your script\n",
    "CLASS_MAP = {\"car\": 0, \"bottle\": 1, \"dog\": 2}  \n",
    "\n",
    "def run_florence_annotation_on_dataloader(\n",
    "    model,\n",
    "    processor,\n",
    "    dataloader,\n",
    "    device,\n",
    "    torch_dtype,\n",
    "    save_label_dir,\n",
    "    log_file_path,\n",
    "    task_prompt=\"<OD>\",\n",
    "    text_input=None,\n",
    "):\n",
    "    os.makedirs(save_label_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        for batch_idx, (images, img_names, original_sizes, img_names) in enumerate(\n",
    "            tqdm(dataloader, desc=\"Running Florence-2 on DataLoader\")\n",
    "        ):\n",
    "            for idx in range(images.size(0)):\n",
    "                image_tensor = images[idx].to(device, torch_dtype)\n",
    "                orig_w, orig_h = original_sizes[idx]\n",
    "                img_path = img_names[idx]\n",
    "                if isinstance(img_path, list):\n",
    "                    img_path = img_path[0]\n",
    "                # print(img_path)\n",
    "                img_base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                label_path = os.path.join(save_label_dir, f\"{img_base}.txt\")\n",
    "\n",
    "                # Convert tensor to PIL\n",
    "                mean = torch.tensor([0.565, 0.566, 0.529]).view(3, 1, 1).to(device)\n",
    "                std = torch.tensor([0.276, 0.276, 0.298]).view(3, 1, 1).to(device)\n",
    "                image_tensor = image_tensor * std + mean\n",
    "                image_tensor = torch.clamp(image_tensor, 0, 1)\n",
    "                image_pil = transforms.ToPILImage()(image_tensor.cpu())\n",
    "\n",
    "                # Build Florence input\n",
    "                prompt = task_prompt + (text_input or \"\")\n",
    "                inputs = processor(text=prompt, images=image_pil, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "                # Run Florence-2 generation\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    pixel_values=inputs[\"pixel_values\"],\n",
    "                    max_new_tokens=1024,\n",
    "                    num_beams=3,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                )\n",
    "\n",
    "                transition_beam_scores = model.compute_transition_scores(\n",
    "                    sequences=generated_ids.sequences,\n",
    "                    scores=generated_ids.scores,\n",
    "                    beam_indices=generated_ids.beam_indices,\n",
    "                )\n",
    "\n",
    "                parsed = processor.post_process_generation(\n",
    "                    sequence=generated_ids.sequences[0],\n",
    "                    transition_beam_score=transition_beam_scores[0],\n",
    "                    task=\"<OD>\",\n",
    "                    image_size=(orig_w, orig_h)\n",
    "                )\n",
    "\n",
    "                od_result = parsed.get(\"<OD>\", {})\n",
    "                bboxes = od_result.get(\"bboxes\", [])\n",
    "                labels = od_result.get(\"labels\", [])\n",
    "\n",
    "                # Save YOLO-format label file\n",
    "                with open(label_path, 'w') as f:\n",
    "                    if not bboxes:\n",
    "                        print(f\"[!] No objects found in {img_base}. Skipping.\")\n",
    "                    for i in range(len(bboxes)):\n",
    "                        label_name = labels[i].lower()\n",
    "                        cls_id = CLASS_MAP.get(label_name, -1)\n",
    "                        if cls_id == -1:\n",
    "                            print(f\"[!] Unknown label '{label_name}' in {img_base}. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        x1, y1, x2, y2 = bboxes[i]\n",
    "                        cx = ((x1 + x2) / 2) / orig_w\n",
    "                        cy = ((y1 + y2) / 2) / orig_h\n",
    "                        bw = (x2 - x1) / orig_w\n",
    "                        bh = (y2 - y1) / orig_h\n",
    "\n",
    "                        f.write(f\"{cls_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "\n",
    "                readable_save_dir = os.path.join(save_label_dir, \"..\", \"detailed_labels\")\n",
    "                os.makedirs(readable_save_dir, exist_ok=True)\n",
    "                readable_path = os.path.join(readable_save_dir, f\"{img_base}.txt\")\n",
    "\n",
    "                with open(readable_path, 'w') as f_readable:\n",
    "                    if not bboxes:\n",
    "                        f_readable.write(\"[!] No objects found.\\n\")\n",
    "                    for i in range(len(bboxes)):\n",
    "                        label_name = labels[i]\n",
    "                        cls_id = CLASS_MAP.get(label_name.lower(), -1)\n",
    "                        if cls_id == -1:\n",
    "                            continue\n",
    "                        x1, y1, x2, y2 = bboxes[i]\n",
    "                        score = od_result.get(\"scores\", [0.0] * len(bboxes))[i]\n",
    "                        # No square brackets, clean and space-separated\n",
    "                        f_readable.write(f\"{cls_id} {label_name} {x1:.2f} {y1:.2f} {x2:.2f} {y2:.2f} {score:.4f}\\n\")\n",
    "\n",
    "        \n",
    "                # Log full parsed output\n",
    "                # Log parsed output in readable format\n",
    "                log_file.write(f\"Image: {img_base}.jpg\\n\")\n",
    "                scores = od_result.get(\"scores\", [])\n",
    "                for i in range(len(bboxes)):\n",
    "                    label_name = labels[i]\n",
    "                    label_lower = label_name.lower()\n",
    "                    cls_id = CLASS_MAP.get(label_lower, -1)\n",
    "                    if cls_id == -1:\n",
    "                        continue\n",
    "                    bbox = bboxes[i]\n",
    "                    score = scores[i] if i < len(scores) else 0.0\n",
    "                    bbox_str = \"[\" + \", \".join(f\"{coord:.2f}\" for coord in bbox) + \"]\"\n",
    "                    log_file.write(f\"{cls_id} {label_name} {bbox_str} {score:.2f}\\n\")\n",
    "                log_file.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:24:30.109315Z",
     "iopub.status.busy": "2025-06-12T18:24:30.108723Z",
     "iopub.status.idle": "2025-06-12T18:24:39.660562Z",
     "shell.execute_reply": "2025-06-12T18:24:39.659836Z",
     "shell.execute_reply.started": "2025-06-12T18:24:30.109292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "run_florence_annotation_on_dataloader(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    dataloader=data_loader, \n",
    "    device=device,\n",
    "    torch_dtype=torch.float16, \n",
    "    save_label_dir=SAVE_LABEL_PATH,\n",
    "    log_file_path=LOG_FILE_PATH,\n",
    "    task_prompt=\"<CAPTION_TO_PHRASE_GROUNDING>\",  \n",
    "    text_input=TEXT_INPUT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:24:44.599518Z",
     "iopub.status.busy": "2025-06-12T18:24:44.599251Z",
     "iopub.status.idle": "2025-06-12T18:24:44.609368Z",
     "shell.execute_reply": "2025-06-12T18:24:44.608644Z",
     "shell.execute_reply.started": "2025-06-12T18:24:44.599500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_annotations(image_dir, label_dir, class_map, image_exts={'.jpg', '.png', '.jpeg'}, max_images=None):\n",
    "    \"\"\"\n",
    "    Plot bounding boxes from YOLO-format labels on images.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to folder containing images.\n",
    "        label_dir (str): Path to folder containing YOLO label files.\n",
    "        class_map (dict): Class index to name mapping.\n",
    "        image_exts (set): Valid image extensions.\n",
    "        max_images (int): Max number of images to visualize (optional).\n",
    "    \"\"\"\n",
    "    image_files = [f for f in sorted(os.listdir(image_dir)) if os.path.splitext(f)[1].lower() in image_exts]\n",
    "    if max_images:\n",
    "        image_files = image_files[:max_images]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Load image\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        # Load corresponding label file\n",
    "        label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "        label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    cls_id, cx, cy, bw, bh = map(float, parts)\n",
    "                    x1 = int((cx - bw / 2) * w)\n",
    "                    y1 = int((cy - bh / 2) * h)\n",
    "                    x2 = int((cx + bw / 2) * w)\n",
    "                    y2 = int((cy + bh / 2) * h)\n",
    "\n",
    "                    bboxes.append((x1, y1, x2, y2))\n",
    "                    labels.append(class_map.get(int(cls_id), str(int(cls_id))))\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.imshow(image)\n",
    "        for bbox, label in zip(bboxes, labels):\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     linewidth=1.5, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1, label, color='black', fontsize=8,\n",
    "                    bbox=dict(facecolor='yellow', alpha=0.7))\n",
    "        ax.set_title(image_file)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:26:45.656246Z",
     "iopub.status.busy": "2025-06-12T18:26:45.655640Z",
     "iopub.status.idle": "2025-06-12T18:26:51.013818Z",
     "shell.execute_reply": "2025-06-12T18:26:51.013038Z",
     "shell.execute_reply.started": "2025-06-12T18:26:45.656223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_annotations(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    label_dir=SAVE_LABEL_PATH,\n",
    "    class_map=CLASS_MAP_LABEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:28:18.531511Z",
     "iopub.status.busy": "2025-06-12T18:28:18.531241Z",
     "iopub.status.idle": "2025-06-12T18:28:18.552736Z",
     "shell.execute_reply": "2025-06-12T18:28:18.552001Z",
     "shell.execute_reply.started": "2025-06-12T18:28:18.531493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "IOU_THRESHOLD = 0.5\n",
    "NUM_CLASSES = 3 # Make sure this matches your dataset's number of classes\n",
    "\n",
    "# IMPORTANT: Set your image dimensions here. \n",
    "# This is crucial for converting YOLO ground truth coordinates correctly.\n",
    "DEFAULT_IMAGE_WIDTH = 250 # Replace with your actual image width\n",
    "DEFAULT_IMAGE_HEIGHT = 250 # Replace with your actual image height\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "# Function to compute IoU between two bounding boxes [x1, y1, x2, y2]\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    if inter_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area\n",
    "\n",
    "# Convert YOLO normalized (xc, yc, w, h) to absolute (xmin, ymin, xmax, ymax)\n",
    "def yolo_normalized_to_xyxy(xc_norm, yc_norm, w_norm, h_norm, img_width, img_height):\n",
    "    xc = xc_norm * img_width\n",
    "    yc = yc_norm * img_height\n",
    "    w = w_norm * img_width\n",
    "    h = h_norm * img_height\n",
    "    \n",
    "    xmin = xc - w / 2\n",
    "    ymin = yc - h / 2\n",
    "    xmax = xc + w / 2\n",
    "    ymax = yc + h / 2\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "# Parse label file (handles both YOLO GT and custom prediction format)\n",
    "def parse_label_file(filepath, is_gt=False, img_width=DEFAULT_IMAGE_WIDTH, img_height=DEFAULT_IMAGE_HEIGHT):\n",
    "    boxes = []\n",
    "    if not os.path.exists(filepath):\n",
    "        return boxes\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            if is_gt:\n",
    "                # YOLO GT format: class_id xc_norm yc_norm w_norm h_norm\n",
    "                class_id = int(parts[0])\n",
    "                # Convert normalized YOLO to absolute XYXY\n",
    "                bbox_xyxy = yolo_normalized_to_xyxy(\n",
    "                    float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4]),\n",
    "                    img_width, img_height\n",
    "                )\n",
    "                boxes.append({'class_id': class_id, 'bbox': bbox_xyxy})\n",
    "            else:\n",
    "                # Prediction format: class_id class_name x1 y1 x2 y2 confidence_score\n",
    "                # Note: 'class_name' (parts[1]) is skipped as it's not used in evaluation\n",
    "                class_id = int(parts[0])\n",
    "                x1 = float(parts[2])\n",
    "                y1 = float(parts[3])\n",
    "                x2 = float(parts[4])\n",
    "                y2 = float(parts[5])\n",
    "                confidence = float(parts[6])\n",
    "                boxes.append({'class_id': class_id, 'bbox': [x1, y1, x2, y2], 'confidence': confidence})\n",
    "    return boxes\n",
    "\n",
    "# Function to calculate Average Precision (AP) for a single class\n",
    "def calculate_ap(detections, num_ground_truths):\n",
    "    # Sort detections by confidence in descending order\n",
    "    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "    TP = np.zeros(len(detections))\n",
    "    FP = np.zeros(len(detections))\n",
    "\n",
    "    for d_idx, detection in enumerate(detections):\n",
    "        if detection['is_tp']:\n",
    "            TP[d_idx] = 1\n",
    "        else:\n",
    "            FP[d_idx] = 1\n",
    "\n",
    "    cumulative_TP = np.cumsum(TP)\n",
    "    cumulative_FP = np.cumsum(FP)\n",
    "\n",
    "    precision = cumulative_TP / (cumulative_TP + cumulative_FP)\n",
    "    recall = cumulative_TP / num_ground_truths if num_ground_truths > 0 else np.zeros_like(cumulative_TP)\n",
    "\n",
    "    # For calculation of AP, we extend the PR curve points\n",
    "    m_rec = np.concatenate(([0.], recall, [recall[-1] + 1e-6 if len(recall) > 0 else 1.])) # Ensure recall goes to 1\n",
    "    m_pre = np.concatenate(([0.], precision, [0.])) # Precision at recall=1 is 0\n",
    "\n",
    "    # Ensure precision is monotonically decreasing (standard for COCO-style AP)\n",
    "    for i in range(len(m_pre) - 2, -1, -1):\n",
    "        m_pre[i] = max(m_pre[i], m_pre[i+1])\n",
    "\n",
    "    # Calculate AP as the area under the PR curve using trapezoidal rule\n",
    "    ap = np.sum((m_rec[1:] - m_rec[:-1]) * m_pre[1:])\n",
    "    \n",
    "    # Return AP and the final precision/recall (corresponding to IOU_THRESHOLD)\n",
    "    final_precision = precision[-1] if len(precision) > 0 else 0\n",
    "    final_recall = recall[-1] if len(recall) > 0 else 0\n",
    "    \n",
    "    return ap, final_precision, final_recall\n",
    "\n",
    "# --- Main Evaluation Function ---\n",
    "def evaluate_metrics(pred_dir, gt_dir, num_classes=NUM_CLASSES, \n",
    "                     img_width=DEFAULT_IMAGE_WIDTH, img_height=DEFAULT_IMAGE_HEIGHT):\n",
    "    \n",
    "    all_detections_per_class = defaultdict(list) \n",
    "    all_ground_truths_per_class = defaultdict(int)\n",
    "\n",
    "    gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith(\".txt\")])\n",
    "    \n",
    "    for file in tqdm(gt_files, desc=\"Processing Images\"):\n",
    "        gt_path = os.path.join(gt_dir, file)\n",
    "        pred_path = os.path.join(pred_dir, file) # Predictions often have same filename\n",
    "\n",
    "        # Parse ground truth and prediction boxes for the current image\n",
    "        gt_boxes = parse_label_file(gt_path, is_gt=True, img_width=img_width, img_height=img_height)\n",
    "        pred_boxes = parse_label_file(pred_path, is_gt=False) \n",
    "\n",
    "        # Keep track of which ground truth boxes have been matched in this image\n",
    "        gt_matched_in_image = [False] * len(gt_boxes)\n",
    "\n",
    "        # First, count all ground truths for this image into the global tally\n",
    "        for gt_box in gt_boxes:\n",
    "            all_ground_truths_per_class[gt_box['class_id']] += 1\n",
    "\n",
    "        # Process predictions, ordered by confidence (highest first)\n",
    "        for pred_box in sorted(pred_boxes, key=lambda x: x['confidence'], reverse=True):\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            # Find the best matching ground truth for the current prediction\n",
    "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                if gt_box['class_id'] == pred_box['class_id'] and not gt_matched_in_image[gt_idx]:\n",
    "                    iou = compute_iou(pred_box['bbox'], gt_box['bbox'])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "\n",
    "            # Determine if the prediction is a True Positive or False Positive\n",
    "            if best_iou >= IOU_THRESHOLD and best_gt_idx != -1:\n",
    "                all_detections_per_class[pred_box['class_id']].append({\n",
    "                    'confidence': pred_box['confidence'],\n",
    "                    'is_tp': True\n",
    "                })\n",
    "                gt_matched_in_image[best_gt_idx] = True # Mark ground truth as matched\n",
    "            else:\n",
    "                all_detections_per_class[pred_box['class_id']].append({\n",
    "                    'confidence': pred_box['confidence'],\n",
    "                    'is_tp': False\n",
    "                })\n",
    "\n",
    "    # --- Calculate Per-Class and Overall Metrics ---\n",
    "    \n",
    "    per_class_precision = {}\n",
    "    per_class_recall = {}\n",
    "    per_class_f1 = {}\n",
    "    aps = [] # Average Precisions for each class\n",
    "\n",
    "    total_tp_overall = 0\n",
    "    total_fp_overall = 0\n",
    "    total_fn_overall = 0\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        num_gt_for_class = all_ground_truths_per_class[cls]\n",
    "        detections_for_class = all_detections_per_class[cls]\n",
    "        \n",
    "        # Calculate AP for the current class\n",
    "        if num_gt_for_class > 0 or len(detections_for_class) > 0:\n",
    "            ap_val, final_prec, final_rec = calculate_ap(detections_for_class, num_gt_for_class)\n",
    "            aps.append(ap_val)\n",
    "        else:\n",
    "            # If no GT or predictions for a class, AP is 0, and other metrics are 0\n",
    "            ap_val, final_prec, final_rec = 0.0, 0.0, 0.0\n",
    "            aps.append(0.0)\n",
    "\n",
    "        # Store per-class metrics at the IOU_THRESHOLD (from the end of PR curve)\n",
    "        per_class_precision[cls] = final_prec\n",
    "        per_class_recall[cls] = final_rec\n",
    "        per_class_f1[cls] = (2 * final_prec * final_rec) / (final_prec + final_rec) if (final_prec + final_rec) > 0 else 0\n",
    "\n",
    "        # Accumulate for overall metrics at IOU_THRESHOLD\n",
    "        class_tps = sum(1 for d in detections_for_class if d['is_tp'])\n",
    "        class_fps = sum(1 for d in detections_for_class if not d['is_tp'])\n",
    "        class_fns = num_gt_for_class - class_tps # Number of ground truths not matched\n",
    "\n",
    "        total_tp_overall += class_tps\n",
    "        total_fp_overall += class_fps\n",
    "        total_fn_overall += class_fns\n",
    "        \n",
    "\n",
    "    # Calculate Mean Average Precision (mAP)\n",
    "    mean_ap = sum(aps) / num_classes if num_classes > 0 else 0.0\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_precision = total_tp_overall / (total_tp_overall + total_fp_overall) if (total_tp_overall + total_fp_overall) > 0 else 0\n",
    "    overall_recall = total_tp_overall / (total_tp_overall + total_fn_overall) if (total_tp_overall + total_fn_overall) > 0 else 0\n",
    "    overall_f1 = (2 * overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    overall_accuracy = total_tp_overall / (total_tp_overall + total_fp_overall + total_fn_overall) if (total_tp_overall + total_fp_overall + total_fn_overall) > 0 else 0\n",
    "\n",
    "\n",
    "    return (per_class_precision, per_class_recall, per_class_f1, \n",
    "            overall_precision, overall_recall, overall_f1, overall_accuracy, mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:31:24.048521Z",
     "iopub.status.busy": "2025-06-12T18:31:24.048267Z",
     "iopub.status.idle": "2025-06-12T18:31:24.077236Z",
     "shell.execute_reply": "2025-06-12T18:31:24.076448Z",
     "shell.execute_reply.started": "2025-06-12T18:31:24.048502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred_dir =  os.path.join(SAVE_LABEL_PATH, \"..\", \"detailed_labels\") \n",
    "gt_dir = LABEL_DIR\n",
    "# Call the evaluation function\n",
    "per_class_prec, per_class_rec, per_class_f1, \\\n",
    "overall_prec, overall_rec, overall_f1, overall_acc, mAP = \\\n",
    "evaluate_metrics(pred_dir, gt_dir, NUM_CLASSES, DEFAULT_IMAGE_WIDTH, DEFAULT_IMAGE_HEIGHT)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Per-Class Metrics (at IoU=0.5) ---\")\n",
    "for cls in range(NUM_CLASSES):\n",
    "    print(f\"Class {cls}: Precision={per_class_prec[cls]:.3f}, Recall={per_class_rec[cls]:.3f}, F1-Score={per_class_f1[cls]:.3f}\")\n",
    "\n",
    "print(\"\\n--- Overall Metrics (aggregated at IoU=0.5) ---\")\n",
    "print(f\"Overall Precision: {overall_prec:.3f}\")\n",
    "print(f\"Overall Recall: {overall_rec:.3f}\")\n",
    "print(f\"Overall F1-Score: {overall_f1:.3f}\")\n",
    "print(f\"Overall Accuracy (TP/(TP+FP+FN)): {overall_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nMean Average Precision (mAP@0.5): {mAP:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T18:33:03.164192Z",
     "iopub.status.busy": "2025-06-12T18:33:03.163861Z",
     "iopub.status.idle": "2025-06-12T18:33:03.600319Z",
     "shell.execute_reply": "2025-06-12T18:33:03.599529Z",
     "shell.execute_reply.started": "2025-06-12T18:33:03.164172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !zip -r file.zip /kaggle/working\n",
    "# !ls\n",
    "# from IPython.display import FileLink\n",
    "# FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7631178,
     "sourceId": 12119621,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7638154,
     "sourceId": 12129599,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7649291,
     "sourceId": 12145413,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7649793,
     "sourceId": 12146021,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
